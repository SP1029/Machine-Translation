{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovfgb76-NT1K"
      },
      "source": [
        "# Acknowledgements\n",
        "https://stackexchange.com/ \\\\\n",
        "https://stackoverflow.com/ \\\\\n",
        "https://discuss.pytorch.org/ <br />\n",
        "https://docs.python.org/3/library/re.html <br />\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html <br />\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html <br />\n",
        "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html <br />\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html <br />\n",
        "https://github.com/lkulowski/LSTM_encoder_decoder <br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re82OXR8SfFY"
      },
      "source": [
        "# README\n",
        "For running the code ensure you have done the following\n",
        "- Update **TEST_START** and **TEST_END** in **Constants** in **Modules** if different testing data is used\n",
        "- Update the **Path Variables** appropriately (ensure for folders the path ends by /\n",
        "- Keep all the files related to model, in the **base_path** folder\n",
        "- Ensure GPU runtime is selected\n",
        "- Download Model Files for testing from https://drive.google.com/drive/folders/1b6cNGYgks1eqrdprbLMQTd2Sd85Ka9cR?usp=drive_link\n",
        "- Before submitting file on Codalab ensure to remove quotes from the title of the csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2FSuqcCQt6A"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk99SDFnNwrK"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkfHpDOgNU4A"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import csv\n",
        "!pip install dill\n",
        "import dill\n",
        "from tqdm import tqdm as time_bar\n",
        "import random\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv8e3L5LN3Cw"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XonSCqEN-ug"
      },
      "source": [
        "### Constants\n",
        "- The contents of TEST_START and TEST_END would have to be updated to store the starting id and the ending id of each language pair in the testing data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUl3mXsdN7nV"
      },
      "outputs": [],
      "source": [
        "# Constants - Please Fill in the values for TEST_START and TEST_END denoting the ids range for each language in correct ordet\n",
        "\n",
        "TEST_START = [177039, 318808, 540139, 683553, 835928, 1001446, 1133590]\n",
        "TEST_END = [196710, 332374, 563223, 696923, 851373, 1018120, 1146420]\n",
        "\n",
        "# Languages are Bengali, Gujarati, Hindi, Kannada, Malyalam, Tamil and Telugu\n",
        "\n",
        "LANGUAGE_PAIRS = [\"English-Bengali\", \"English-Gujarati\", \"English-Hindi\", \"English-Kannada\", \"English-Malayalam\", \"English-Tamil\", \"English-Telgu\"]\n",
        "\n",
        "# Train:Test Split Ratio\n",
        "TEST_TRAIN_SPLIT = 0.9\n",
        "\n",
        "# Dimensions of the word embedding\n",
        "WORD_EM_DIM = 300\n",
        "\n",
        "# Encoding type of the json\n",
        "ENC_TYPE = \"UTF-8\"\n",
        "\n",
        "# Reference\n",
        "# Wikipedia Unicode Blocks\n",
        "\n",
        "# Punctuation in various languages (which usually is not part of simple words)\n",
        "EN_PUNCT = ['~', '`', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '}', '[', '],', '|', '\\\\', ':', '\\\"', ';', '\\'', '<', '>', '?', ',', '.', '/']\n",
        "EN_NUM = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
        "UNI_SYM = ['\\u2013', '\\u2014', '\\u2015', '\\u2017', '\\u2018', '\\u2019', '\\u201A', '\\u201B', '\\u201C', '\\u201D', '\\u201E', '\\u2020', '\\u2021', '\\u2022', '\\u2026', '\\u2030', '\\u2032', '\\u2033', '\\u2039', '\\u203A', '\\u203C', '\\u203E', '\\u2044', '\\u204A']\n",
        "BE_PUNCT = ['\\u09F7']\n",
        "BE_NUM = ['\\u09E6', '\\u09E7', '\\u09E8', '\\u09E9', '\\u09EA', '\\u09EB', '\\u09EC', '\\u09ED', '\\u09EE', '\\u09EF']\n",
        "GU_PUNCT = []\n",
        "GU_NUM = ['\\u0AE6', '\\u0AE7', '\\u0AE8', '\\u0AE9', '\\u0AEA', '\\u0AEB', '\\u0AEC', '\\u0AED', '\\u0AEE', '\\u0AEF']\n",
        "HI_PUNCT = ['\\u0964','\\u0965']\n",
        "HI_NUM = ['\\u0966', '\\u0967', '\\u0968', '\\u0969', '\\u096A', '\\u096B', '\\u096C', '\\u096D', '\\u096E', '\\u096F']\n",
        "KA_PUNCT = []\n",
        "KA_NUM = ['\\u0CE6', '\\u0CE7', '\\u0CE8', '\\u0CE9', '\\u0CEA', '\\u0CEB', '\\u0CEC', '\\u0CED', '\\u0CEE', '\\u0CEF']\n",
        "MA_PUNCT = []\n",
        "MA_NUM = ['\\u0D66', '\\u0D67', '\\u0D68', '\\u0D69', '\\u0D6A', '\\u0D6B', '\\u0D6C', '\\u0D6D', '\\u0D6E', '\\u0D6F']\n",
        "TA_PUNCT = []\n",
        "TA_NUM = ['\\u0BE6', '\\u0BE7', '\\u0BE8', '\\u0BE9', '\\u0BEA', '\\u0BEB', '\\u0BEC', '\\u0BED', '\\u0BEE', '\\u0BEF', '\\u0BF0', '\\u0BF1', '\\u0BF1']\n",
        "TE_PUNCT = []\n",
        "TE_NUM = ['\\u0C66', '\\u0C67', '\\u0C68', '\\u0C69', '\\u0C6A', '\\u0C6B', '\\u0C6C', '\\u0C6D', '\\u0C6E', '\\u0C6F']\n",
        "\n",
        "# Combination of all above\n",
        "TOK_PUNCT = EN_PUNCT + EN_NUM + UNI_SYM + BE_PUNCT + BE_NUM + GU_PUNCT + GU_NUM + HI_PUNCT + HI_NUM + KA_PUNCT + KA_NUM + MA_PUNCT + MA_NUM + TA_PUNCT + TA_NUM + TE_PUNCT + TE_NUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQvSRV_ORjo"
      },
      "source": [
        "### Boolean Representation\n",
        "- For Converting Sentences into one-hot representations for having faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWfN47ntNERl"
      },
      "outputs": [],
      "source": [
        "# Boolise Data\n",
        "\n",
        "# Boolean Sentences\n",
        "class Boolise_Data:\n",
        "    def __init__(self, voc):\n",
        "        self.voc = voc\n",
        "        self.col = voc.size\n",
        "\n",
        "    def boolise_data(self, data):\n",
        "        '''\n",
        "        Input as tokenised sentences\n",
        "        '''\n",
        "\n",
        "        row = len(data)\n",
        "        data_bin = torch.zeros([row,self.col], dtype=torch.bool)\n",
        "        for ix_sen, sen in enumerate(data):\n",
        "            for word in sen:\n",
        "                ix_word = self.voc.get_ix(word)\n",
        "                data_bin[ix_sen][ix_word] = True\n",
        "\n",
        "        return data_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMe2le_4OW5I"
      },
      "source": [
        " ### Data Loaders\n",
        " - For Loading Data from json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTUjivc6NERu"
      },
      "outputs": [],
      "source": [
        "# Data Loaders\n",
        "\n",
        "class Dataloader:\n",
        "    def __init__(self, file_path, language_pair):\n",
        "        self.file_path = file_path\n",
        "        self.language_pair = language_pair\n",
        "        self.json_data = None\n",
        "        self.ids = []\n",
        "        self.source_data = []\n",
        "        self.target_data = []\n",
        "\n",
        "    def load_json(self):\n",
        "        with open(self.file_path, 'r', encoding=ENC_TYPE) as file:\n",
        "            self.json_data = json.load(file)\n",
        "\n",
        "    def json_to_list(self, is_train):\n",
        "        for language_pair, language_data in self.json_data.items():\n",
        "            if language_pair==self.language_pair:\n",
        "                for data_type, data_entries in language_data.items():\n",
        "                    for entry_id, entry_data in data_entries.items():\n",
        "                        self.ids.append(entry_id)\n",
        "                        self.source_data.append(entry_data[\"source\"])\n",
        "                        if is_train==True:\n",
        "                            self.target_data.append(entry_data[\"target\"])\n",
        "\n",
        "    def get_data(self, is_train):\n",
        "        self.load_json()\n",
        "        self.json_to_list(is_train)\n",
        "        if is_train:\n",
        "            return self.ids, self.source_data, self.target_data\n",
        "        else:\n",
        "            return self.ids, self.source_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi4FFyBOObGL"
      },
      "source": [
        "### Pre-Trained Word Embeddings\n",
        "- Loads them initially while creating vocabulary and provides a binary mark to zero out gradients of pre-trained word-embeddings during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WScCM1OuNERw"
      },
      "outputs": [],
      "source": [
        "# Pre Word\n",
        "\n",
        "class Pre_Word_Em:\n",
        "    def __init__(self, pre_trained_path, em_dim, voc_src):\n",
        "        self.pre_trained_path = pre_trained_path\n",
        "        self.pre_set = set()\n",
        "        self.em_dim = em_dim\n",
        "        self.em_df = pd.read_csv(self.pre_trained_path, quoting=csv.QUOTE_NONE, sep=' ', header=None)\n",
        "        self.word_ix = {}\n",
        "        self.voc_src = voc_src\n",
        "        self.mask = None\n",
        "\n",
        "        # Fill in the set\n",
        "        pre_words = self.em_df.iloc[:, 0].to_numpy()\n",
        "        self.pre_set.update(pre_words)\n",
        "\n",
        "        # Create dict\n",
        "        ix = 0\n",
        "        for word in self.em_df.iloc[:, 0]:\n",
        "            self.word_ix[word] = ix\n",
        "            ix = ix + 1\n",
        "\n",
        "    def is_present(self, word):\n",
        "        '''\n",
        "        return True if word is present in the Pre-Trained Word Embeddings\n",
        "        '''\n",
        "        if word in self.pre_set:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_word_em(self, word):\n",
        "        '''\n",
        "        Return Pre-Trained Word Embedding of a word which has been checked to exist in the Pre Word Em\n",
        "        '''\n",
        "        ix = self.word_ix[word]\n",
        "        return self.em_df.iloc[ix,1:]\n",
        "\n",
        "    def create_mask(self):\n",
        "        self.mask = torch.ones(len(self.voc_src.word_ix), 1 ,dtype=torch.bool)\n",
        "\n",
        "        for word,ix in self.voc_src.word_ix.items():\n",
        "            if self.is_present(word):\n",
        "                self.mask[ix] = False\n",
        "\n",
        "    def get_mask(self):\n",
        "        if self.mask==None:\n",
        "            self.create_mask()\n",
        "\n",
        "        return self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm8AfOKGO7hn"
      },
      "source": [
        "### Sentence Vectoriser\n",
        "- Used for getting the vector representation of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVVsmAVuNERx"
      },
      "outputs": [],
      "source": [
        "# Sentence Vectoriser\n",
        "\n",
        "class Sentence_Vectoriser:\n",
        "    '''Used to create a vector representation of a sentence'''\n",
        "\n",
        "    def __init__ (self, voc, word_em):\n",
        "        self.vocabulary = voc\n",
        "        self.word_em = word_em\n",
        "\n",
        "    def vectorise_sentence(self, sen):\n",
        "        '''Vectorises the sentence and return the vector'''\n",
        "\n",
        "        # Calculate sentence vector\n",
        "        sen_vec = torch.zeros(1, WORD_EM_DIM)\n",
        "\n",
        "        for word in sen:\n",
        "            ix = self.vocabulary.get_ix(word)\n",
        "\n",
        "            # Ignore unknown words\n",
        "            if ix!=-1:\n",
        "                sen_vec = sen_vec + self.word_em.get_word_em(ix)\n",
        "\n",
        "        # Average\n",
        "        sen_vec = sen_vec / len(sen)\n",
        "\n",
        "        return sen_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV0aszqaPFWL"
      },
      "source": [
        "### Model 10\n",
        "- The code of the Model 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TWrBOe7NERy"
      },
      "outputs": [],
      "source": [
        "# Model 10\n",
        "\n",
        "class Model:\n",
        "    '''\n",
        "    To learn bring the word embeddings to the same vector space\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data_src, data_tok_src, data_tok_tgt, voc_src, voc_tgt, pre_word, is_src):\n",
        "        self.is_src = is_src\n",
        "        self.data_src = data_src\n",
        "        self.data_tok_src = data_tok_src\n",
        "        self.data_tok_tgt = data_tok_tgt\n",
        "        self.we_src = voc_src.word_em\n",
        "        self.we_tgt = voc_tgt.word_em\n",
        "        self.voc_src = voc_src\n",
        "        self.voc_tgt = voc_tgt\n",
        "        self.sen_vectoriser_src = Sentence_Vectoriser(voc_src, voc_src.word_em)\n",
        "        self.sen_vectoriser_tgt = Sentence_Vectoriser(voc_tgt, voc_tgt.word_em)\n",
        "        self.sample_size = len(data_tok_src)\n",
        "        self.boolise_src = Boolise_Data(voc_src)\n",
        "        self.boolise_tgt = Boolise_Data(voc_tgt)\n",
        "        self.optimiser = torch.optim.Adam(params=(self.we_src.word_em, self.we_tgt.word_em), lr=0.001)\n",
        "        self.translator = Translator(voc_src.word_em, voc_src, voc_tgt.word_em, voc_tgt)\n",
        "        self.pre_word = pre_word\n",
        "\n",
        "    def train(self, epoch, batch_size, sub_epoch):\n",
        "        '''\n",
        "        Do Mini Batch GD for these many epochs\n",
        "        '''\n",
        "\n",
        "        # Get Mask\n",
        "        updation_mask = self.pre_word.get_mask()\n",
        "\n",
        "        for itr in range(epoch):\n",
        "\n",
        "            batch_itr = 0\n",
        "            p_bar = time_bar(total = self.sample_size, position=0, leave=True)\n",
        "            while batch_itr<self.sample_size:\n",
        "                # Zero Grads optimiser\n",
        "                self.optimiser.zero_grad()\n",
        "\n",
        "                ix = batch_itr\n",
        "\n",
        "                if ix+batch_size-1>=self.sample_size:\n",
        "                    break\n",
        "\n",
        "                # Boolise Data\n",
        "                bool_src = self.boolise_src.boolise_data(self.data_tok_src[ix:ix+batch_size])\n",
        "                bool_tgt = self.boolise_tgt.boolise_data(self.data_tok_tgt[ix:ix+batch_size])\n",
        "\n",
        "                # print(ix)\n",
        "                for i in range(sub_epoch):\n",
        "                    # Compute New Sentence Vectors\n",
        "                    data_vec_src = torch.transpose((torch.transpose(bool_src,0,1) / (1 + torch.sum(bool_src, dim=1))),0,1)  @ self.we_src.word_em\n",
        "                    data_vec_tgt = torch.transpose((torch.transpose(bool_tgt ,0,1) / (1 + torch.sum(bool_tgt, dim=1))),0,1) @ self.we_tgt.word_em\n",
        "\n",
        "                    # Compute Loss\n",
        "                    loss_mse = torch.nn.MSELoss()\n",
        "                    loss = loss_mse(data_vec_src, data_vec_tgt)\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Mask the grads\n",
        "                    if self.is_src:\n",
        "                        self.we_src.word_em.grad = self.we_src.word_em.grad * updation_mask\n",
        "                    else:\n",
        "                        self.we_tgt.word_em.grad = self.we_tgt.word_em.grad * updation_mask\n",
        "\n",
        "                    self.optimiser.step()\n",
        "\n",
        "                batch_itr = (batch_itr + batch_size) % self.sample_size\n",
        "\n",
        "                p_bar.update(batch_size)\n",
        "\n",
        "            p_bar.close()\n",
        "        print(loss)\n",
        "        test_ix =  random.randint(0, self.sample_size)\n",
        "        print(str(self.data_src[test_ix ]))\n",
        "        print(self.translator.translate(str(self.data_src[test_ix])),\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F44bsBzxPJ54"
      },
      "source": [
        "### Submission Utility\n",
        "- Combines multiple csv into one csv with proper formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq23qsTHNER0"
      },
      "outputs": [],
      "source": [
        "# Submission - Ensure to remove quotes from headings after saving\n",
        "\n",
        "def combine_csv(folder_path):\n",
        "    '''\n",
        "    Input the Google Drive folder path where all 7 prediction files are stored to combine into answer.csv\n",
        "    '''\n",
        "    # Load Files\n",
        "    df1 = pd.read_csv(folder_path + \"translations_0_English-Bengali.csv\")\n",
        "    df2 = pd.read_csv(folder_path + \"translations_1_English-Gujarati.csv\")\n",
        "    df3 = pd.read_csv(folder_path + \"translations_2_English-Hindi.csv\")\n",
        "    df4 = pd.read_csv(folder_path + \"translations_3_English-Kannada.csv\")\n",
        "    df5 = pd.read_csv(folder_path + \"translations_4_English-Malayalam.csv\")\n",
        "    df6 = pd.read_csv(folder_path + \"translations_5_English-Tamil.csv\")\n",
        "    df7 = pd.read_csv(folder_path + \"translations_6_English-Telgu.csv\")\n",
        "\n",
        "    # Combine and save\n",
        "    df_ans = pd.concat([df1,df2,df3,df4,df5,df6,df7])\n",
        "    df_ans.to_csv(f\"{folder_path}/answer.csv\", index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2lcx0OsPRgM"
      },
      "source": [
        "### Tokeniser\n",
        "- For Tokensiing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3TyRd8CNER1"
      },
      "outputs": [],
      "source": [
        "# Tokensier\n",
        "\n",
        "# Major References\n",
        "# https://stackoverflow.com/questions/30933216/split-by-regex-without-resulting-empty-strings-in-python#:~:text=%3E%3E%3E%20re.findall(r%27%5CS%2B%27%2C%20%27%20a%20b%20%20%20c%20%20de%20%20%27)\n",
        "\n",
        "class Tokeniser:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def tok_sen(self, sen):\n",
        "        '''To tokenise a single input (precisely not a sentence)'''\n",
        "\n",
        "        # To lower\n",
        "        sen = sen.lower()\n",
        "\n",
        "        # Add space before and after punctuations and numbers\n",
        "        for ch in TOK_PUNCT:\n",
        "            if ch in sen:\n",
        "                sen = sen.replace(ch,' '+ ch + ' ')\n",
        "\n",
        "        # Split text at empty characters\n",
        "        return re.findall('\\S+',sen)\n",
        "\n",
        "    def tok_data(self, data):\n",
        "        '''To tokenise data - list of sentences'''\n",
        "\n",
        "        # Tokenise every sentence in the data\n",
        "        tok_data = []\n",
        "        for sen in data:\n",
        "            tok_data.append(self.tok_sen(sen))\n",
        "\n",
        "        return tok_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uQWhYRRPXKn"
      },
      "source": [
        "### Translator\n",
        "- Takes input as source language sentence and outputs the translation in target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJBPu5QgNER4"
      },
      "outputs": [],
      "source": [
        "# Translator\n",
        "\n",
        "# Major References\n",
        "# https://stackoverflow.com/questions/3463746/in-place-string-modifications-in-python\n",
        "\n",
        "class Translator:\n",
        "    '''Translates from language 1 to language 2 given the word embeddings'''\n",
        "\n",
        "    def __init__(self, word_em_1, voc_1, word_em_2, voc_2):\n",
        "        self.word_em_1 = word_em_1\n",
        "        self.word_em_2 = word_em_2\n",
        "        self.voc_1 = voc_1\n",
        "        self.voc_2 = voc_2\n",
        "        self.tokeniser = Tokeniser()\n",
        "\n",
        "    def translate(self, sen):\n",
        "        '''\n",
        "        Translates from language 1 to language 2\n",
        "\n",
        "        Method of translation:\n",
        "        Finds closest word embedding of the word1 in word_em_2\n",
        "        '''\n",
        "\n",
        "        # Vectorise Sen 1\n",
        "        sen_tok = self.tokeniser.tok_sen(sen)\n",
        "\n",
        "        # Find closest word in word_em_2 space for each word in vectorised sentence\n",
        "        translated_sen_words = []\n",
        "        for word_src in sen_tok:\n",
        "            word_src_ix = self.voc_1.get_ix(word_src)\n",
        "            if word_src_ix == -1:\n",
        "                translated_sen_words.append(\"\")\n",
        "                continue\n",
        "\n",
        "            word_src_em = self.word_em_1.get_word_em(word_src_ix)\n",
        "            predicted_word_ix = self.word_em_2.get_nearest_word_ix(word_src_em)\n",
        "            predicted_word = self.voc_2.get_word(predicted_word_ix)\n",
        "            translated_sen_words.append(predicted_word)\n",
        "\n",
        "        # Make the translated sentence\n",
        "        translated_sen = sen.lower()\n",
        "        n = len(sen_tok)\n",
        "        ix = 0\n",
        "        for i in range(n):\n",
        "            word_frm = sen_tok[i]\n",
        "            word_to = translated_sen_words[i]\n",
        "            ix = translated_sen.find(word_frm,ix)\n",
        "            temp = translated_sen[ix:].replace(word_frm, word_to, 1)\n",
        "            translated_sen = translated_sen[:ix] + temp\n",
        "            ix = ix + len(word_to)\n",
        "\n",
        "        return translated_sen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodmLnQSPfxv"
      },
      "source": [
        "### Word Embeddings\n",
        "- To store and get the word embeddings\n",
        "- Also for word-to-word translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t49hWeLNER5"
      },
      "outputs": [],
      "source": [
        "# Word Embeddings\n",
        "\n",
        "class Word_Em:\n",
        "    '''\n",
        "    Class for keeping embeddings\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, pre_word, has_pre, word_em_dim):\n",
        "        if has_pre==True:\n",
        "            self.pre_word = pre_word\n",
        "        self.word_em = None\n",
        "        self.word_em_dim = word_em_dim\n",
        "        self.has_pre = has_pre\n",
        "\n",
        "    def load_word_em(self, word_em_path):\n",
        "        self.word_em = torch.load(word_em_path)\n",
        "\n",
        "    def create_word_em(self, voc):\n",
        "        '''\n",
        "        Create initial random word embedding given the vocabulary\n",
        "        '''\n",
        "        # Init\n",
        "        self.word_em = torch.rand(voc.size, self.word_em_dim, requires_grad=False)\n",
        "        tot_count = voc.size\n",
        "        match_count = 0\n",
        "\n",
        "        # Copy Pre-Trained\n",
        "        if self.has_pre==True:\n",
        "            for word, ix in voc.word_ix.items():\n",
        "                if self.pre_word.is_present(word):\n",
        "                    match_count += 1\n",
        "                    self.word_em[ix] = torch.tensor(np.array(self.pre_word.get_word_em(word)).astype(np.float32))\n",
        "\n",
        "        print(str(match_count), \" words found in pre_word out of \", str(tot_count))\n",
        "\n",
        "        # Update Model\n",
        "        self.word_em.requires_grad = True\n",
        "\n",
        "    def get_word_em(self, ix):\n",
        "        '''Get the word embedding of a word given its index'''\n",
        "\n",
        "        return self.word_em[ix]\n",
        "\n",
        "    def get_nearest_word_ix(self, word_em_src):\n",
        "        '''\n",
        "        Get the word embedding vector ix which is closest to the word_em_src in vector space\n",
        "        '''\n",
        "        # Find the ix of the word\n",
        "        ix = torch.argmin(torch.linalg.norm(self.word_em - word_em_src, dim=1))\n",
        "\n",
        "        # Return the word ix\n",
        "        return int(ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3HFiR6NPoSQ"
      },
      "source": [
        "### Vocabulary\n",
        "- A wrapper for maintaining the vocabulary and word embeddings of the languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwQRBm1gNER6"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "\n",
        "class Vocabulary:\n",
        "    '''Contains the Vocabulary'''\n",
        "\n",
        "    def __init__(self, pre_word, has_pre, word_em_dim):\n",
        "        self.word_ix = {}\n",
        "        self.ix_word = {}\n",
        "        self.voc_set = set()\n",
        "        self.size = 0\n",
        "        self.word_em = Word_Em(pre_word, has_pre, word_em_dim)\n",
        "        self.word_em_dim = word_em_dim\n",
        "        pass\n",
        "\n",
        "    def save(self, file_name):\n",
        "        file = open(file_name, 'wb')\n",
        "        dill.dump(self, file)\n",
        "        file.close()\n",
        "\n",
        "    def create_voc(self, data_tok, pre_made,pre_made_path):\n",
        "        '''Create Vocab from tokenised data'''\n",
        "\n",
        "        # Find unique words\n",
        "        for sen in data_tok:\n",
        "            for word in sen:\n",
        "                self.voc_set.add(str(word))\n",
        "\n",
        "        # Set the size\n",
        "        self.size = len(self.voc_set)\n",
        "\n",
        "        # Create the dictionaries\n",
        "        key = 0\n",
        "        for ix, word in enumerate(self.voc_set):\n",
        "            self.word_ix[word] = ix\n",
        "            self.ix_word[ix] = word\n",
        "            key = key + 1\n",
        "\n",
        "        # Create the word embeddings\n",
        "        if pre_made==True:\n",
        "            self.word_em.load_word_em(pre_made_path)\n",
        "        else:\n",
        "            self.word_em.create_word_em(self)\n",
        "\n",
        "        print(\"Vocabulary has been created\")\n",
        "\n",
        "    def get_word_representation(self, word):\n",
        "        '''Get word given index'''\n",
        "        if word in self.word_ix.keys():\n",
        "            ix = self.word_ix[word]\n",
        "            return self.word_em.get_word_em(ix)\n",
        "        else:\n",
        "            return torch.zeros(self.word_em_dim)\n",
        "\n",
        "\n",
        "    def get_translated_word(self, v):\n",
        "        '''Get translated word given vectorised src'''\n",
        "\n",
        "        ix = self.word_em.get_nearest_word_ix(v)\n",
        "        return self.ix_word[ix]\n",
        "\n",
        "    def get_ix(self, word):\n",
        "        '''Get index given word'''\n",
        "        if word in self.word_ix.keys():\n",
        "            return self.word_ix[word]\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def get_word(self,ix):\n",
        "        '''\n",
        "        Get word given index\n",
        "        '''\n",
        "        return self.ix_word[ix]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IONk92m-P5ZN"
      },
      "source": [
        "### Word Order Scorer\n",
        "- Architecture of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvIi3X1CP4mE"
      },
      "outputs": [],
      "source": [
        "# Word Order Scorer Architecture\n",
        "\n",
        "class NN(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.i12i2 = torch.nn.Linear(input_size, int(input_size/4))\n",
        "        self.i22i3 = torch.nn.Linear(int(input_size/4), int(input_size/16))\n",
        "        self.i32i4 = torch.nn.Linear(int(input_size/16), int(input_size/64))\n",
        "        self.i42i5 = torch.nn.Linear(int(input_size/64), int(input_size/128))\n",
        "        self.i52o = torch.nn.Linear(int(input_size/128), output_size)\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        i1_z = self.i12i2(input)\n",
        "        i2 = self.act(i1_z)\n",
        "        i2_z = self.i22i3(i2)\n",
        "        i3 = self.act(i2_z)\n",
        "        i3_z = self.i32i4(i3)\n",
        "        i4 = self.act(i3_z)\n",
        "        i4_z = self.i42i5(i4)\n",
        "        i5 = self.act(i4_z)\n",
        "        i5_z = self.i52o(i5)\n",
        "        o = self.act(i5_z)\n",
        "        return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMADD2mXQFZH"
      },
      "source": [
        "## Path Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyNlxG2vNER7"
      },
      "outputs": [],
      "source": [
        "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
        "torch.set_default_device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AQS4lnFbD-r"
      },
      "outputs": [],
      "source": [
        "train_data_path = r\"/gdrive/MyDrive/CS779-Machine-Translation/Data/Train-Validation/train_data1.json\"\n",
        "val_data_path = r\"/gdrive/MyDrive/CS779-Machine-Translation/Data/Train-Validation/val_data1.json\"\n",
        "prediction_folder_path = r\"/gdrive/MyDrive/CS779-Machine-Translation/Phase 1/Training/Prediction/\"\n",
        "base_path = r\"/gdrive/MyDrive/CS779-Machine-Translation/Phase 1/Training/Models/Final/\"\n",
        "test_data_path = r\"/gdrive/MyDrive/CS779-Machine-Translation/Data/Train-Validation/test_data1_final.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqcBtm-CbFQf"
      },
      "outputs": [],
      "source": [
        "# Required for conistency with the objects created\n",
        "\n",
        "# Load Custom Modules\n",
        "import sys\n",
        "sys.path.append(base_path)\n",
        "\n",
        "import _load_data\n",
        "importlib.reload(_load_data)\n",
        "from _load_data import Dataloader\n",
        "\n",
        "import _tokeniser\n",
        "importlib.reload(_tokeniser)\n",
        "from _tokeniser import Tokeniser\n",
        "\n",
        "import _translator\n",
        "importlib.reload(_translator)\n",
        "from _translator import Translator\n",
        "\n",
        "import _model\n",
        "importlib.reload(_model)\n",
        "from _model import Model\n",
        "\n",
        "import _constants\n",
        "importlib.reload(_constants)\n",
        "from _constants import LANGUAGE_PAIRS\n",
        "from _constants import WORD_EM_DIM\n",
        "\n",
        "import _vocab\n",
        "importlib.reload(_vocab)\n",
        "from _vocab import Vocabulary\n",
        "\n",
        "import _word_em\n",
        "importlib.reload(_word_em)\n",
        "from _word_em import Word_Em\n",
        "\n",
        "import _submission\n",
        "importlib.reload(_submission)\n",
        "from _submission import combine_csv\n",
        "\n",
        "import _pre_word\n",
        "importlib.reload(_pre_word)\n",
        "from _pre_word import Pre_Word_Em"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Fw5UYMa8ip"
      },
      "source": [
        "- Run if you want to load google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiF6HV1tWhD0"
      },
      "outputs": [],
      "source": [
        "# Load Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdyQvPkJQK1-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HqYqu6OQOFv"
      },
      "source": [
        "## Training Model 10\n",
        "- Note for illustration purposes I have shown training on the models already trained vocabulary, while actually first I had loaded the common words from pre-trained word embeddings into the newly initialised vocabulary and then trained on it but since pre-trained word embedding files are very big several GB I have not included here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzaS_P_eNER8"
      },
      "outputs": [],
      "source": [
        "# Training Model 10 for a single langauge pair (Use this code in 7 different notebooks to speed up training)\n",
        "\n",
        "lp = \"English-Telgu\"\n",
        "\n",
        "# Load Data\n",
        "dl_lp = Dataloader(train_data_path, lp)\n",
        "ids, source_data, target_data = dl_lp.get_data(True)\n",
        "\n",
        "# Tokenise Data\n",
        "tokenizer = Tokeniser()\n",
        "source_data_tk = tokenizer.tok_data(source_data)\n",
        "target_data_tk = tokenizer.tok_data(target_data)\n",
        "\n",
        "# Create Vocabulary\n",
        "with open(base_path + 'tgt_model_voc_tgt_' + lp, 'rb') as f:\n",
        "    voc_tgt = dill.load(f)\n",
        "    voc_tgt.word_em.pre_word = None\n",
        "voc_src = Vocabulary(None, False, WORD_EM_DIM)\n",
        "voc_src.create_voc(source_data_tk, False, None)\n",
        "\n",
        "# Load pre-trained word embeddings mask\n",
        "with open(base_path + 'tgt_pw_' + lp, 'rb') as f:\n",
        "    pw_tgt = dill.load(f)\n",
        "\n",
        "# Random Shuffle\n",
        "data = list(zip(ids, source_data, target_data))\n",
        "random.shuffle(data)\n",
        "ids, source_data, target_data = zip(*data)\n",
        "N = len(source_data)\n",
        "\n",
        "# Train Model\n",
        "model = Model(source_data, source_data_tk, target_data_tk, voc_src, voc_tgt, pw_tgt, False)\n",
        "for i in range(30):\n",
        "  model.train(1,128,10)\n",
        "\n",
        "for i in range(10):\n",
        "  model.train(1,1024,30)\n",
        "\n",
        "# Save the Model's Vocabularies\n",
        "file = open(base_path + \"tgt_model_voc_tgt_\" + lp, 'wb')\n",
        "dill.dump(voc_tgt, file)\n",
        "file.close()\n",
        "\n",
        "file = open(base_path + \"tgt_model_voc_src_\" + lp, 'wb')\n",
        "dill.dump(voc_src, file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1zult7cQRyl"
      },
      "source": [
        "## Training the Word Order Scorer\n",
        "- Note for illustration purposes I have shown training on the models already trained vocabulary, while actually I had trained it on the newly initialised vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODevGfA2NER-"
      },
      "outputs": [],
      "source": [
        "# Train Word Order Scorer for one Language\n",
        "\n",
        "lp = \"English-Kannada\"\n",
        "\n",
        "# Load Data\n",
        "dl_lp = Dataloader(train_data_path, lp)\n",
        "ids, source_data, target_data = dl_lp.get_data(True)\n",
        "\n",
        "# Random Shuffle\n",
        "data = list(zip(ids, source_data, target_data))\n",
        "random.shuffle(data)\n",
        "ids, source_data, target_data = zip(*data)\n",
        "N = len(source_data)\n",
        "print(\"Data Randomly Shuffled\")\n",
        "\n",
        "# Tokenise Data\n",
        "tokenizer = Tokeniser()\n",
        "\n",
        "# Load target vocabulary\n",
        "with open(base_path + 'tgt_model_voc_tgt_' + lp, 'rb') as f:\n",
        "    voc_tgt = dill.load(f)\n",
        "voc_tgt.word_em.word_em.requires_grad = False\n",
        "\n",
        "# Generate Training Data\n",
        "train_data = []\n",
        "train_data_word = []\n",
        "for i in time_bar(range(len(target_data_tk))):\n",
        "    sen = target_data_tk[i]\n",
        "    for j in range(len(sen)):\n",
        "        if j==len(sen)-1:\n",
        "            continue\n",
        "        word1 = sen[j]\n",
        "        word2 = sen[j+1]\n",
        "\n",
        "        ix1 = voc_tgt.get_ix(word1)\n",
        "        ix2 = voc_tgt.get_ix(word2)\n",
        "\n",
        "        if ix1!=-1 and ix2!=-1:\n",
        "            train_data.append([[word1, word2], torch.tensor([1], dtype=torch.float)])\n",
        "            train_data.append([[word2, word1], torch.tensor([0], dtype=torch.float)])\n",
        "            train_data_word.append([[word1,word2],1])\n",
        "            train_data_word.append([[word2,word1],0])\n",
        "\n",
        "# Load Model\n",
        "with open(base_path + 'word_ordered_' + lp, 'rb') as f:\n",
        "    nn = dill.load(f)\n",
        "\n",
        "# Train\n",
        "optimiser = torch.optim.Adam(params = nn.parameters())\n",
        "epochs = 1\n",
        "for e in range(epochs):\n",
        "    for k in time_bar(range(len(train_data))):\n",
        "        optimiser.zero_grad()\n",
        "        ip_data = train_data[k][0]\n",
        "        we1 = voc_tgt.get_word_representation(ip_data[0])\n",
        "        we2 = voc_tgt.get_word_representation(ip_data[1])\n",
        "        ip = torch.concat([we1,we2])\n",
        "        pred = nn(ip)\n",
        "        loss = torch.nn.MSELoss()\n",
        "        l = loss(pred, train_data[k][1])\n",
        "        l.backward()\n",
        "        optimiser.step()\n",
        "        if k%10000==0:\n",
        "            print(\"\\n\",l)\n",
        "        if k%100000==0:\n",
        "          file = open(base_path + \"word_ordered_\" + lp, 'wb')\n",
        "          dill.dump(nn, file)\n",
        "          file.close()\n",
        "\n",
        "# Save\n",
        "file = open(base_path + \"word_ordered_\" + lp, 'wb')\n",
        "dill.dump(nn, file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgGTpPeTQQ82"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdkdRglUNER_"
      },
      "outputs": [],
      "source": [
        "# Model 10 and Final Model Testing\n",
        "\n",
        "for lp_ix, lp in enumerate(LANGUAGE_PAIRS):\n",
        "\n",
        "    # Create Vocabulary\n",
        "    with open(base_path + 'tgt_model_voc_tgt_' + lp, 'rb') as f:\n",
        "        voc_tgt = dill.load(f)\n",
        "    with open(base_path + 'tgt_model_voc_src_' + lp, 'rb') as f:\n",
        "        voc_src = dill.load(f)\n",
        "\n",
        "    # Load Test Data\n",
        "    dl_lp = Dataloader(test_data_path, lp)\n",
        "    ids, source_data = dl_lp.get_data(False)\n",
        "\n",
        "    # Translate for Test Data\n",
        "    print(\"Translating Model 10 for \", lp)\n",
        "    ts = Translator(voc_src.word_em, voc_src, voc_tgt.word_em, voc_tgt)\n",
        "    translations = []\n",
        "    for ix, sen in enumerate(source_data):\n",
        "      translated_sen = ts.translate(sen)\n",
        "      translations.append({\"ID\": ids[ix], \"Translation\": translated_sen})\n",
        "\n",
        "    translations_df = pd.DataFrame(translations)\n",
        "    translations_df.to_csv(prediction_folder_path + f\"translations_{lp}.csv\", index=False)\n",
        "\n",
        "# Saving Predictions of Model 10\n",
        "combine_csv(prediction_folder_path)\n",
        "\n",
        "# Final Model Predictions\n",
        "df = pd.read_csv(prediction_folder_path+ \"answer.csv\",sep=\"\\t\", index_col=\"ID\")\n",
        "tokenizer = Tokeniser()\n",
        "\n",
        "for lp_ix, lp in enumerate(LANGUAGE_PAIRS):\n",
        "\n",
        "    print(\"Translating Final Model for\", lp)\n",
        "\n",
        "    # Loading the Vocabulary\n",
        "    with open(base_path + \"word_ordered_\" + lp, 'rb') as f:\n",
        "        nn = dill.load(f)\n",
        "    with open(base_path + \"tgt_model_voc_tgt_\" + lp, 'rb') as f:\n",
        "        voc_tgt = dill.load(f)\n",
        "    voc_tgt.word_em.word_em.requires_grad = False\n",
        "\n",
        "    # Running the model for each sentence in prediction\n",
        "    for i in time_bar(range(TEST_START[lp_ix], TEST_END[lp_ix]+1)):\n",
        "\n",
        "        # Tokenise Sentence\n",
        "        sen = str(df.loc[i][0])\n",
        "        sen_tok_new = tokenizer.tok_sen(sen)\n",
        "        x = len(sen_tok_new)\n",
        "\n",
        "        # Repeat process until all pairs of adjacent words are in order\n",
        "        while True:\n",
        "            found = False\n",
        "            for j in range(x-1):\n",
        "\n",
        "                # Get a pair of adj words\n",
        "                word1 = sen_tok_new[j]\n",
        "                word2 = sen_tok_new[j+1]\n",
        "\n",
        "                # Skip the pair if they are Puctuations / Numbers or same words\n",
        "                if word1 in TOK_PUNCT or word2 in TOK_PUNCT or word1==word2:\n",
        "                    continue\n",
        "                ix1 = voc_tgt.get_ix(word1)\n",
        "                ix2 = voc_tgt.get_ix(word2)\n",
        "\n",
        "                if ix1!=-1 and ix2!=-1:\n",
        "                    we1 = voc_tgt.get_word_representation(word1)\n",
        "                    we2 = voc_tgt.get_word_representation(word2)\n",
        "                    pred = nn(torch.cat([we2,we1]))\n",
        "\n",
        "                    # If score of reverse order greater than threshold then swap\n",
        "                    if float(pred)>0.81:\n",
        "                        Found = True\n",
        "                        sen_tok_new[j], sen_tok_new[j+1] = sen_tok_new[j+1], sen_tok_new[j]\n",
        "\n",
        "                        # Tokenise sen\n",
        "                        sen_tok = tokenizer.tok_sen(sen)\n",
        "\n",
        "                        # Translate\n",
        "                        translated_sen_words = sen_tok_new\n",
        "\n",
        "                        # Make the translated sentence\n",
        "                        translated_sen = sen.lower()\n",
        "                        n = len(sen_tok)\n",
        "                        ix = 0\n",
        "                        for k in range(n):\n",
        "                            word_frm = sen_tok[k]\n",
        "                            word_to = translated_sen_words[k]\n",
        "                            ix = translated_sen.find(word_frm,ix)\n",
        "                            temp = translated_sen[ix:].replace(word_frm, word_to, 1)\n",
        "                            translated_sen = translated_sen[:ix] + temp\n",
        "                            ix = ix + len(word_to)\n",
        "                            sen = translated_sen\n",
        "            if found==False:\n",
        "                # print(sen,\"\\n\")\n",
        "                df.loc[i][0] = sen\n",
        "                break\n",
        "\n",
        "# Save new predictions\n",
        "df.to_csv(prediction_folder_path + \"answer_new.csv\", quotechar='\"', quoting=csv.QUOTE_NONNUMERIC, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeRHHaOGYt3X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
